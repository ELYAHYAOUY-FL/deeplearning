{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f10798",
   "metadata": {},
   "source": [
    "# objectif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfaaa67",
   "metadata": {},
   "source": [
    "The main purpose behind this lab is to get familiar with Pytorch, to build deep neural network architecture for Natural language process by using Sequence Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcc2fd",
   "metadata": {},
   "source": [
    "# Part1 Classification Task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ec0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cbc92b9",
   "metadata": {},
   "source": [
    "##   qst 1 :collect text data from several Arabic web site from scrapping  using libraries BeautifulSoup , try to concerning ARTIFICIAL INTELEGENCE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd29afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde38bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Score\n",
      "0  تعريف الذكاء الاصطناعي - موضوع التصنيفات أجدد ...      9\n",
      "1  خصائص الذكاء الاصطناعي - موضوع التصنيفات أجدد ...      7\n",
      "2  مجالات الذكاء الاصطناعي - موضوع التصنيفات أجدد...      8\n",
      "3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...      7\n",
      "4  بحث عن مخاطر الإنترنت - موضوع التصنيفات أجدد ا...      6\n",
      "5  خصائص الذكاء - موضوع التصنيفات أجدد المقالات ا...      0\n",
      "6  من هو مخترع الكهرباء - موضوع التصنيفات أجدد ال...      4\n",
      "7  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...      2\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://mawdoo3.com/%D8%AA%D8%B9%D8%B1%D9%8A%D9%81_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
    "    \"https://mawdoo3.com/%D8%AE%D8%B5%D8%A7%D8%A6%D8%B5_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
    "    \"https://mawdoo3.com/%D9%85%D8%AC%D8%A7%D9%84%D8%A7%D8%AA_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
    "    \"https://innovationhub.social/articles/impact17_01\",\n",
    "    \"https://mawdoo3.com/%D8%A8%D8%AD%D8%AB_%D8%B9%D9%86_%D9%85%D8%AE%D8%A7%D8%B7%D8%B1_%D8%A7%D9%84%D8%A5%D9%86%D8%AA%D8%B1%D9%86%D8%AA\",\n",
    "    \"https://mawdoo3.com/%D8%AE%D8%B5%D8%A7%D8%A6%D8%B5_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1\",\n",
    "    \"https://mawdoo3.com/%D9%85%D9%86_%D9%87%D9%88_%D9%85%D8%AE%D8%AA%D8%B1%D8%B9_%D8%A7%D9%84%D9%83%D9%87%D8%B1%D8%A8%D8%A7%D8%A1\",\n",
    "    \"https://mawdoo3.com/%D8%A3%D9%87%D9%85%D9%8A%D8%A9_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A_%D9%81%D9%8A_%D9%85%D8%AC%D8%A7%D9%84_%D8%A7%D9%84%D8%AA%D8%B9%D9%84%D9%8A%D9%85\"\n",
    "]\n",
    "\n",
    "texts = []\n",
    "\n",
    "# Loop through URLs and scrape data\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract all text from the webpage\n",
    "        content = soup.get_text(separator=' ', strip=True)\n",
    "        texts.append(content)\n",
    "    else:\n",
    "        print(f\"Failed to fetch URL: {url}\")\n",
    "\n",
    "# Manually assign scores based on relevance (example)\n",
    "scores = [9, 7, 8, 7, 6, 0, 4, 2, ]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Text': texts, 'Score': scores})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecebfdba",
   "metadata": {},
   "source": [
    "## qst 2 : preprocessing NLP pipeline (tokenization stemming lemmatization, Stop words, Discretization.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c81fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Original Text  \\\n",
      "0  تعريف الذكاء الاصطناعي - موضوع التصنيفات أجدد ...   \n",
      "1  خصائص الذكاء الاصطناعي - موضوع التصنيفات أجدد ...   \n",
      "2  مجالات الذكاء الاصطناعي - موضوع التصنيفات أجدد...   \n",
      "3  الذكاء الاصطناعي في خدمة التنمية المستدامة - م...   \n",
      "4  بحث عن مخاطر الإنترنت - موضوع التصنيفات أجدد ا...   \n",
      "5  خصائص الذكاء - موضوع التصنيفات أجدد المقالات ا...   \n",
      "6  من هو مخترع الكهرباء - موضوع التصنيفات أجدد ال...   \n",
      "7  أهمية الذكاء الاصطناعي في مجال التعليم - موضوع...   \n",
      "\n",
      "                                      Processed Text  \\\n",
      "0  تعريف الذكاء الاصطناعي موضوع التصنيفات أجدد ال...   \n",
      "1  خصائص الذكاء الاصطناعي موضوع التصنيفات أجدد ال...   \n",
      "2  مجالات الذكاء الاصطناعي موضوع التصنيفات أجدد ا...   \n",
      "3  الذكاء الاصطناعي خدمة التنمية المستدامة مبادرة...   \n",
      "4  بحث مخاطر الإنترنت موضوع التصنيفات أجدد المقال...   \n",
      "5  خصائص الذكاء موضوع التصنيفات أجدد المقالات الأ...   \n",
      "6  مخترع الكهرباء موضوع التصنيفات أجدد المقالات ا...   \n",
      "7  أهمية الذكاء الاصطناعي مجال التعليم موضوع التص...   \n",
      "\n",
      "                                 Tokenized Sequences  Score  \n",
      "0  [39, 1, 2, 3, 46, 47, 48, 32, 49, 3, 117, 118,...      9  \n",
      "1  [9, 1, 2, 3, 46, 47, 48, 32, 49, 3, 117, 118, ...      7  \n",
      "2  [50, 1, 2, 3, 46, 47, 48, 32, 49, 3, 117, 118,...      8  \n",
      "3  [1, 2, 107, 102, 191, 1615, 1616, 92, 1617, 16...      7  \n",
      "4  [79, 82, 10, 3, 46, 47, 48, 32, 49, 3, 117, 11...      6  \n",
      "5  [9, 1, 3, 46, 47, 48, 32, 49, 3, 117, 118, 138...      0  \n",
      "6  [38, 4, 3, 46, 47, 48, 32, 49, 3, 117, 118, 13...      4  \n",
      "7  [22, 1, 2, 11, 25, 3, 46, 47, 48, 32, 49, 3, 1...      2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/hp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer, lemmatizer, and tokenizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Define Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = ''.join([char for char in text if char not in string.punctuation and not char.isdigit()])\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in arabic_stopwords]\n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "    return lemmatized\n",
    "\n",
    "# Preprocess texts\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Convert processed texts back into strings for tokenization\n",
    "processed_strings = [' '.join(text) for text in processed_texts]\n",
    "\n",
    "# Tokenization (fit tokenizer and transform texts into sequences)\n",
    "tokenizer.fit_on_texts(processed_strings)\n",
    "sequences = tokenizer.texts_to_sequences(processed_strings)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "dfProcess = pd.DataFrame({\n",
    "    'Original Text': texts,\n",
    "    'Processed Text': processed_strings,\n",
    "    'Tokenized Sequences': sequences,\n",
    "    'Score': scores\n",
    "})\n",
    "\n",
    "print(dfProcess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e805",
   "metadata": {},
   "source": [
    "## qst 3 :Train OUR models by using RNN, Bidirectional RNN GRU and LSTM Architectures and tuning hyper-parameters to get the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27016f5c",
   "metadata": {},
   "source": [
    "### prepareing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33940220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scaling the scores between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "scaled_scores = scaler.fit_transform(np.array(scores).reshape(-1, 1)).flatten()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_strings, scaled_scores, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer preparation\n",
    "max_vocab = 10000  # Limit vocabulary size\n",
    "max_len = 100      # Maximum sequence length\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57581e",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520e27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Bidirectional, GRU, LSTM\n",
    "\n",
    "# Common hyperparameters\n",
    "embedding_dim = 64\n",
    "units = 128  # Number of RNN units\n",
    "output_dim = 1  # For regression; use `output_dim = num_classes` for classification\n",
    "\n",
    "# Simple RNN\n",
    "def build_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        SimpleRNN(units, return_sequences=False),\n",
    "        Dense(output_dim, activation='sigmoid')  # For regression; use 'softmax' for classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c76e3",
   "metadata": {},
   "source": [
    "### Bidirectional RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526b91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_rnn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        Bidirectional(SimpleRNN(units, return_sequences=False)),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb033916",
   "metadata": {},
   "source": [
    "### GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68bcaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        GRU(units, return_sequences=False),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3723d",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab785f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        LSTM(units, return_sequences=False),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cef2600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 751ms/step - loss: 0.1109 - mae: 0.2876 - val_loss: 0.2114 - val_mae: 0.4012\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0384 - mae: 0.1614 - val_loss: 0.2514 - val_mae: 0.4124\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0172 - mae: 0.1084 - val_loss: 0.3037 - val_mae: 0.4685\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0043 - mae: 0.0572 - val_loss: 0.3663 - val_mae: 0.5435\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0084 - mae: 0.0780 - val_loss: 0.3235 - val_mae: 0.4869\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0081 - mae: 0.0694 - val_loss: 0.3057 - val_mae: 0.4991\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0036 - mae: 0.0431 - val_loss: 0.2585 - val_mae: 0.4579\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0092 - mae: 0.0887 - val_loss: 0.2945 - val_mae: 0.4477\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0022 - mae: 0.0416 - val_loss: 0.3126 - val_mae: 0.4551\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0017 - mae: 0.0377 - val_loss: 0.3192 - val_mae: 0.4635\n"
     ]
    }
   ],
   "source": [
    "# Training the Simple RNN model\n",
    "rnn_model = build_rnn_model()\n",
    "history = rnn_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbdbeb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3192 - mae: 0.4635\n",
      "Test Loss: 0.3191853165626526, Test MAE: 0.463494211435318\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "loss, mae = rnn_model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217053fd",
   "metadata": {},
   "source": [
    "### Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "443fcd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m98.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/hp/.local/lib/python3.10/site-packages (from keras-tuner) (2.31.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: keras in /home/hp/.local/lib/python3.10/site-packages (from keras-tuner) (2.15.0)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hp/.local/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hp/.local/lib/python3.10/site-packages (from requests->keras-tuner) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hp/.local/lib/python3.10/site-packages (from requests->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hp/.local/lib/python3.10/site-packages (from requests->keras-tuner) (3.4)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2876a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 01s]\n",
      "val_mae: 0.36290690302848816\n",
      "\n",
      "Best val_mae So Far: 0.0965351015329361\n",
      "Total elapsed time: 00h 00m 37s\n",
      "Best hyperparameters: {'embedding_dim': 64, 'units': 64, 'learning_rate': 0.01, 'tuner/epochs': 10, 'tuner/initial_epoch': 4, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0013'}\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import Hyperband\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, \n",
    "                  output_dim=hp.Choice('embedding_dim', [32, 64, 128]), \n",
    "                  input_length=max_len),\n",
    "        SimpleRNN(hp.Choice('units', [64, 128, 256]), return_sequences=False),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice('learning_rate', [0.01, 0.001, 0.0001])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_epochs=10,\n",
    "    directory='tuning_results',\n",
    "    project_name='rnn_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_pad, y_train, validation_data=(X_test_pad, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e506705a",
   "metadata": {},
   "source": [
    "### Testing and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3eae060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1290 - mae: 0.3039 - val_loss: 0.1953 - val_mae: 0.3888\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0296 - mae: 0.1542 - val_loss: 0.1947 - val_mae: 0.3867\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0079 - mae: 0.0769 - val_loss: 0.1729 - val_mae: 0.3940\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0093 - mae: 0.0821 - val_loss: 0.2075 - val_mae: 0.4342\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0051 - mae: 0.0605 - val_loss: 0.1941 - val_mae: 0.3340\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0134 - mae: 0.0969 - val_loss: 0.1965 - val_mae: 0.3950\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0023 - mae: 0.0393 - val_loss: 0.2393 - val_mae: 0.4243\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0010 - mae: 0.0218 - val_loss: 0.2543 - val_mae: 0.4329\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0019 - mae: 0.0340 - val_loss: 0.2635 - val_mae: 0.4408\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0010 - mae: 0.0257 - val_loss: 0.2762 - val_mae: 0.4444\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0985 - mae: 0.2793 - val_loss: 0.1663 - val_mae: 0.3889\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0948 - mae: 0.2755 - val_loss: 0.1692 - val_mae: 0.3889\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0914 - mae: 0.2717 - val_loss: 0.1726 - val_mae: 0.3889\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0880 - mae: 0.2676 - val_loss: 0.1766 - val_mae: 0.3889\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0847 - mae: 0.2632 - val_loss: 0.1815 - val_mae: 0.3889\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0814 - mae: 0.2583 - val_loss: 0.1875 - val_mae: 0.3889\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0781 - mae: 0.2527 - val_loss: 0.1952 - val_mae: 0.3889\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0751 - mae: 0.2462 - val_loss: 0.2050 - val_mae: 0.3889\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0724 - mae: 0.2386 - val_loss: 0.2176 - val_mae: 0.3889\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0706 - mae: 0.2298 - val_loss: 0.2333 - val_mae: 0.3889\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0973 - mae: 0.2773 - val_loss: 0.1659 - val_mae: 0.3889\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0947 - mae: 0.2747 - val_loss: 0.1682 - val_mae: 0.3889\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0920 - mae: 0.2719 - val_loss: 0.1713 - val_mae: 0.3889\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0890 - mae: 0.2686 - val_loss: 0.1754 - val_mae: 0.3889\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0855 - mae: 0.2643 - val_loss: 0.1817 - val_mae: 0.3888\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0814 - mae: 0.2585 - val_loss: 0.1920 - val_mae: 0.3888\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0765 - mae: 0.2498 - val_loss: 0.2109 - val_mae: 0.3886\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0716 - mae: 0.2357 - val_loss: 0.2525 - val_mae: 0.3883\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0708 - mae: 0.2234 - val_loss: 0.2751 - val_mae: 0.3884\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0734 - mae: 0.2200 - val_loss: 0.2561 - val_mae: 0.3887\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Model MAE: 0.46349417501025725\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "Model MAE: 0.4444005125098758\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "Model MAE: 0.38889106445842314\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7899e630cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 209ms/step\n",
      "Model MAE: 0.3886615865760379\n",
      "Best model is LSTM with MAE=0.3886615865760379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"Model MAE: {mae}\")\n",
    "    return mae\n",
    "\n",
    "# Instantiate the Bidirectional RNN model\n",
    "bidirectional_rnn_model = build_bidirectional_rnn_model()\n",
    "\n",
    "# Train the Bidirectional RNN model\n",
    "history_bidir = bidirectional_rnn_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "def build_gru_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        GRU(units, return_sequences=False),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_vocab, output_dim=embedding_dim, input_length=max_len),\n",
    "        LSTM(units, return_sequences=False),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build, train, and evaluate GRU model\n",
    "gru_model = build_gru_model()\n",
    "gru_history = gru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Build, train, and evaluate LSTM model\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate all models\n",
    "rnn_mae = evaluate_model(rnn_model, X_test_pad, y_test)\n",
    "bidir_mae = evaluate_model(bidirectional_rnn_model, X_test_pad, y_test)\n",
    "gru_mae = evaluate_model(gru_model, X_test_pad, y_test)\n",
    "lstm_mae = evaluate_model(lstm_model, X_test_pad, y_test)\n",
    "\n",
    "# Compare results\n",
    "model_results = {\n",
    "    \"RNN\": rnn_mae,\n",
    "    \"Bidirectional RNN\": bidir_mae,\n",
    "    \"GRU\": gru_mae,\n",
    "    \"LSTM\": lstm_mae\n",
    "}\n",
    "\n",
    "best_model_name = min(model_results, key=model_results.get)\n",
    "print(f\"Best model is {best_model_name} with MAE={model_results[best_model_name]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069469a4",
   "metadata": {},
   "source": [
    "## qst 4 :4. Evaluate the four languages models by using standards metrics and other metrics like blue score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "640e867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Simple RNN - Mean Squared Error: 25.854003965505512\n",
      "Simple RNN - Mean Absolute Error: 4.171447515487671\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Bidirectional RNN - Mean Squared Error: 22.369710471772237\n",
      "Bidirectional RNN - Mean Absolute Error: 3.99960446357727\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "GRU - Mean Squared Error: 18.895463987935045\n",
      "GRU - Mean Absolute Error: 3.500019550323486\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "LSTM - Mean Squared Error: 20.741301534421837\n",
      "LSTM - Mean Absolute Error: 3.497954368591308\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# Define a function to evaluate the model\n",
    "def evaluate_model(model, X_test_pad, y_test, model_name=\"Model\"):\n",
    "    # Predict the scores for the test set\n",
    "    y_pred = model.predict(X_test_pad)\n",
    "    \n",
    "    # If using scaling, convert predictions and y_test back to the original scale\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{model_name} - Mean Squared Error: {mse}\")\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"{model_name} - Mean Absolute Error: {mae}\")\n",
    "    \n",
    "    # If task is sequence generation, compute BLEU score (only if y_test and y_pred are text)\n",
    "    # Uncomment the following lines if using BLEU score for sequence generation\n",
    "    if isinstance(y_test[0], str):  # Only compute BLEU if y_test contains text\n",
    "        bleu_scores = []\n",
    "        for i in range(len(y_test)):\n",
    "            reference = [y_test[i].split()]  # Reference sequence (split into words)\n",
    "            candidate = y_pred[i].split()  # Candidate sequence (split into words)\n",
    "            bleu_score = sentence_bleu(reference, candidate)\n",
    "            bleu_scores.append(bleu_score)\n",
    "        print(f\"{model_name} - BLEU Score: {sum(bleu_scores) / len(bleu_scores)}\")\n",
    "\n",
    "# Example usage for evaluation\n",
    "evaluate_model(rnn_model, X_test_pad, y_test, model_name=\"Simple RNN\")\n",
    "\n",
    "# Evaluate the Bidirectional RNN model\n",
    "evaluate_model(bidirectional_rnn_model, X_test_pad, y_test, model_name=\"Bidirectional RNN\")\n",
    "\n",
    "# Evaluate the GRU model\n",
    "evaluate_model(gru_model, X_test_pad, y_test, model_name=\"GRU\")\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "evaluate_model(lstm_model, X_test_pad, y_test, model_name=\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bc224",
   "metadata": {},
   "source": [
    "# Part 2 Transformer (Text generation): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51e03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset_dir = \"jokes_data\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "dataset_path = os.path.join(dataset_dir, \"shortjokes.csv\")\n",
    "\n",
    "jokes = [\n",
    "    \"Why don't scientists trust atoms? Because they make up everything!\",\n",
    "    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n",
    "    \"What do you get when you cross a snowman with a vampire? Frostbite.\",\n",
    "    \"Why don’t skeletons fight each other? They don’t have the guts.\",\n",
    "    \"I told my wife she should embrace her mistakes. She gave me a hug.\",\n",
    "    \"How do you organize a space party? You planet.\",\n",
    "    \"What’s orange and sounds like a parrot? A carrot.\"\n",
    "]\n",
    "\n",
    "with open(dataset_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"joke\"])\n",
    "    for i, joke in enumerate(jokes):\n",
    "        writer.writerow([i, joke])\n",
    "\n",
    "# Load dataset\n",
    "def load_jokes(file_path):\n",
    "    jokes_data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            jokes_data.append({\"text\": row[\"joke\"]})\n",
    "    return jokes_data\n",
    "\n",
    "data = load_jokes(dataset_path)\n",
    "dataset = Dataset.from_dict({\"text\": [d[\"text\"] for d in data]})\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=50)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e29164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048b62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b326f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Why don't scientists trust atoms?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,  # Creativity level\n",
    "    top_p=0.9,        # Top-p sampling for diversity\n",
    "    do_sample=True,   # Enable sampling\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
