# Deep Learning Lab: Natural Language Processing with Sequence Models and Transformers

## **Objective**
The primary goal of this lab is to gain hands-on experience with PyTorch for building deep learning models in the context of Natural Language Processing (NLP). This includes using sequence models and transformers for tasks like text classification and text generation.

---

## **Work Completed**

### **Part 1: Text Classification**
1. **Data Collection**:
   - Collected text data from Arabic websites using web scraping tools (`BeautifulSoup`).
   - Structured the dataset with relevance scores (ranging from 0 to 10).

2. **NLP Preprocessing Pipeline**:
   - Tokenized the collected texts.
   - Applied stemming, lemmatization, and removed stop words.
   - Discretized text data for compatibility with deep learning models.

3. **Deep Learning Models**:
   - Trained the following sequence models:
     - Recurrent Neural Networks (RNN)
     - Bidirectional RNN
     - Gated Recurrent Units (GRU)
     - Long Short-Term Memory (LSTM)
   - Tuned hyperparameters to improve performance.

4. **Evaluation**:
   - Evaluated models using standard metrics like accuracy, F1 score, precision, and recall.
   - Used BLEU score to assess the quality of generated texts.

---

### **Part 2: Text Generation with Transformers**
1. **Pre-Trained Transformer (GPT-2)**:
   - Installed PyTorch-Transformers library.
   - Fine-tuned GPT-2 on a custom dataset.

2. **Text Generation**:
   - Generated meaningful paragraphs from given sentences using the fine-tuned GPT-2 model.

---

## **What I Learned**
- **Data Collection**:
  - Gained proficiency in web scraping techniques using libraries like `BeautifulSoup`.
  - Learned to structure raw data into a labeled dataset for training machine learning models.

- **NLP Preprocessing**:
  - Understood the significance of preprocessing steps like tokenization, stemming, lemmatization, and stop word removal.
  - Observed how discretization can improve input compatibility with deep learning models.

- **Deep Learning for NLP**:
  - Acquired knowledge of building and training sequence models like RNN, GRU, and LSTM.
  - Explored the strengths and weaknesses of different architectures for text classification.

- **Evaluation Techniques**:
  - Learned to evaluate models using metrics like accuracy, precision, recall, F1 score, and BLEU score for both classification and text generation tasks.

- **Transformers**:
  - Gained insights into transformer-based models like GPT-2 for text generation.
  - Learned to fine-tune pre-trained models to adapt them to custom datasets.


---


